= Roma : Convert a CSV file to SQL Insert Statements
Hildeberto Mendonca <me@hildeberto.com>
v0.0.0
:doctype: book
:pdf-page-size: LETTER
:encoding: utf-8
:toc: left
:toclevels: 3
:numbered:

> Roma is a command line tool written in Rust to convert a CSV file to SQL statements. It has special features like grouping insert statements in transaction chunks and inserting multiple rows with a single insert statement.

== Why Roma

Using cloud providers to run our business has been a game changer. They allow us to rapidly provision all the resources to ensure high availability, security, and performance. In many cases, They also manage resources for us, considerably reducing complexity and saving time. However, the more control we delegate to them the less autonomy we have over those resources. This is a good thing because it pushes towards the adoption of good practices, but it can also be challenging sometimes.

Take the provision of a managed PostgreSQL Server on Azure, for example. Azure ensures it is optimally configured for our needs, but it is quite limited when it comes to seeding the database using a CSV file. This is relatively easy doing with direct access to the server, but not possible to do in a managed instance.

To solve this problem to all platforms, all cloud providers, and all SQL databases, we have developed a simple tool capable of converting a CSV file into a SQL file, where rows turn into insert statements. All you need after using Roma is a mean to execute the generated SQL script.

== Using Rome

The https://waterloo.ca[City of Waterloo], located in Ontario, Canadá, has an https://data.waterloo.ca[Open Data Portal]. It publishes raw data about infrastructure, services, environment, transportation, etc. Residents can use the data to oversee public investments and services, identify gaps, discover development opportunities, and even create new business. For us, this portal is useful to test Roma. There is an interesting dataset we want to start with: an https://data.waterloo.ca/datasets/street-tree-inventory[inventory of every single tree planted in the streets] of Waterloo.

image::images/waterloo_tree_inventory.png[Waterloo Tree Inventory]

This dataset is available in the folder https://github.com/htmfilho/roma/tree/main/examples[/examples]. We can process it with the following command:

    $ ./roma --csv waterloo_tree_inventory.csv

which generates the sql file `waterloo_tree_inventory.sql`. This is what happens by default:

- the name of the CSV file is used as the name of the table in the insert statement
- the first line is skipped because it contains the headers that describe the data
- the headers in the first line are used as columns of the table
- the column separator is comma
- each line in the CSV turns into an insert statement
- if the value contains at least one alphanumeric character then it is quoted.
- if the value contains a valid number then it is not quoted.

=== Supported Arguments

These default behaviours can change using arguments. For example, to set a table name different from the name of the CSV file, use:

    $ ./roma --csv waterloo_tree_inventory.csv --table TREE_INVENTORY

If the first line contains data instead of headers then indicate it using `--headers` or `-h`:

    --headers false

If the file is using `tab` or `semicolon` characters as value delimiters, instead of `comma`, then specify it with `--delimiter` or `-d`:

    --delimiter tab

If you need to put some SQL statements or documentation before the generated statements, at the beginning of the file, use the argument `--prefix` or `-p`:

    --prefix waterloo_tree_inventory_prefix.sql

We also support putting content at the end of the file, using the argument `--suffix` or `-s`:

    --suffix waterloo_tree_inventory_suffix.sql

=== Pending Arguments

There are some arguments that we want to develop in the near future. We ended up documenting them first.

If the headers are not present in the file or they don't match the columns of the table then we can customize the columns, passing multiple `--column` or `-c` arguments, one for each column:

    --column coord_x --column coord_y --column street --column species ...

Note that the `--column` argument is required if `--headers` is `false`.

Sometimes the CSV file is too large and generates too many insert statements to the point a single database transaction cannot cope. To create several thransactions throughout the file, use:

    --chunk 1000

It puts chunks of 1000 inserts between `begin transaction` and `commit transaction`. The insert statements can be further optimized, inserting several records at once by using:

    --chunkinsert 250

A proper configuration of `--chunk` and `chunkinsert` can optimize the SQL file maximum performance. In this case, a CSV file with 10000 records would be converted into a SQL with 10 transactions and each transaction would contain 4 inserts with 250 records each.